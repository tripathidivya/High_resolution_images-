{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-13T06:46:49.670807Z","iopub.execute_input":"2023-09-13T06:46:49.671098Z","iopub.status.idle":"2023-09-13T06:46:55.184422Z","shell.execute_reply.started":"2023-09-13T06:46:49.671072Z","shell.execute_reply":"2023-09-13T06:46:55.182863Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport math","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:46:55.186568Z","iopub.execute_input":"2023-09-13T06:46:55.187619Z","iopub.status.idle":"2023-09-13T06:47:05.931437Z","shell.execute_reply.started":"2023-09-13T06:46:55.187581Z","shell.execute_reply":"2023-09-13T06:47:05.930286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the image data, low res image data/ High res image data from the dataset\nMain_img_folder =  '../input/image-super-resolution-from-unsplash/Image Super Resolution - Unsplash'\nlow_res_img = os.path.join(Main_img_folder, 'low res')\nhigh_res_img = os.path.join(Main_img_folder, 'high res') ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:05.932813Z","iopub.execute_input":"2023-09-13T06:47:05.933630Z","iopub.status.idle":"2023-09-13T06:47:05.938673Z","shell.execute_reply.started":"2023-09-13T06:47:05.933597Z","shell.execute_reply":"2023-09-13T06:47:05.937719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read .csv file of image data\ndata = pd.read_csv(\"../input/image-super-resolution-from-unsplash/Image Super Resolution - Unsplash/image_data.csv\")\n\n#takes each element in the 'low_res' column of the 'data' DataFrame, assumes that these elements are filenames or paths, and joins them with the 'low_res_img' path to create complete file paths.\ndata['low_res']= [os.path.join(low_res_img, x) for x in data['low_res']]\n\n#takes each element in the 'low_res' column of the 'data' DataFrame, assumes that these elements are filenames or paths, and joins them with the 'high_res_img' path to create complete file paths.\ndata['high_res']= [os.path.join(high_res_img, x) for x in data['high_res']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:05.941440Z","iopub.execute_input":"2023-09-13T06:47:05.942474Z","iopub.status.idle":"2023-09-13T06:47:06.006039Z","shell.execute_reply.started":"2023-09-13T06:47:05.942439Z","shell.execute_reply":"2023-09-13T06:47:06.005032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4 mages will be loaded and processed together during each iteration of training or validation.\nbatch_size = 4 ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:06.007378Z","iopub.execute_input":"2023-09-13T06:47:06.007776Z","iopub.status.idle":"2023-09-13T06:47:06.013182Z","shell.execute_reply.started":"2023-09-13T06:47:06.007743Z","shell.execute_reply":"2023-09-13T06:47:06.012112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resceld and split the train and validation data\ndatagen_low=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\ndatagen_high=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n\n# create a genearator for the given dataframe for low & High resolution train and validation data for High and Low resolution\nlow_res_generator_train=datagen_low.flow_from_dataframe(\n        data,\n        x_col='low_res',\n        target_size=(800, 1200),\n        class_mode = None,\n        batch_size = batch_size,\n        seed=42,\n        subset='training')\n\nhigh_res_generator_train=datagen_high.flow_from_dataframe(\n        data,\n        x_col='high_res',\n        target_size=(800, 1200),\n        class_mode = None,\n        batch_size = batch_size,\n        seed=42,\n        subset='training')\n\nlow_res_generator_val=datagen_low.flow_from_dataframe(\n        data,\n        x_col='low_res',\n        target_size=(800, 1200),\n        class_mode = None,\n        batch_size = batch_size,\n        seed=42,# means image loading and preprocessing process is reproducible.\n        subset='validation')\nhigh_res_generator_val=datagen_high.flow_from_dataframe(\n        data,\n        x_col='high_res',\n        target_size=(800, 1200),\n        class_mode = None,# dealing with image regression (predicting continuous values for high-resolution images), set class_mode to None, indicating that we are not using any class labels.\n        batch_size = batch_size,\n        seed=42,\n        subset='validation')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:06.015009Z","iopub.execute_input":"2023-09-13T06:47:06.015740Z","iopub.status.idle":"2023-09-13T06:47:08.494770Z","shell.execute_reply.started":"2023-09-13T06:47:06.015707Z","shell.execute_reply":"2023-09-13T06:47:08.493694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Zipped all the train and test data into 2 different files\ntrain_image_zip = zip(low_res_generator_train, high_res_generator_train)\nval_image_zip = zip(low_res_generator_val, high_res_generator_val)\n\n#This code will create a list of tuples containing (low_res, hi_res) pairs from the train_image_zip iterable\ndef imageGenerator(train_image_zip):\n    for (low_res, hi_res) in train_image_zip:\n            yield (low_res, hi_res)\n    #return [(low_res, hi_res) for (low_res, hi_res) in train_image_zip]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:08.497851Z","iopub.execute_input":"2023-09-13T06:47:08.498323Z","iopub.status.idle":"2023-09-13T06:47:08.506097Z","shell.execute_reply.started":"2023-09-13T06:47:08.498287Z","shell.execute_reply":"2023-09-13T06:47:08.505149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nn = 0\n#fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n\nfor img, out in train_image_zip:\n    if n < 5:\n        fig, axes = plt.subplots(1, 2, figsize=(30, 10))\n        axes[0].clear()\n        axes[0].set_title('High Resolution Image', color='green', fontsize=20)\n        axes[0].imshow(out[0])\n        axes[0].axis('off')\n\n        axes[1].clear()\n        axes[1].set_title('Low Resolution Image', color='black', fontsize=20)\n        axes[1].imshow(img[0])\n        axes[1].axis('off')\n\n        plt.pause(0.1)  # Pause briefly to allow the figure to update\n        plt.show()\n\n        n += 1\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:08.509448Z","iopub.execute_input":"2023-09-13T06:47:08.509734Z","iopub.status.idle":"2023-09-13T06:47:20.247768Z","shell.execute_reply.started":"2023-09-13T06:47:08.509709Z","shell.execute_reply":"2023-09-13T06:47:20.246737Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_img = tf.keras.layers.Input(shape=(800, 1200, 3))\n\n# l1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(input_img)\n# l2 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l1)\n# l3 = tf.keras.layers.MaxPool2D(padding='same')(l2)\n\n# l4 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l3)\n# l5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l4)\n# l6 = tf.keras.layers.MaxPool2D(padding='same')(l5)\n\n# l7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l6)\n\n# l8 = tf.keras.layers.UpSampling2D()(l7)\n# l9 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l8)\n# l10 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l9)\n\n# l11 = tf.keras.layers.add([l10, l5])\n\n# l12 = tf.keras.layers.UpSampling2D()(l11)\n# l13 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l12)\n# l14 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l13)\n\n# l15 = tf.keras.layers.add([l14, l2])\n\n# decoded_image = tf.keras.layers.Conv2D(3, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-10))(l15)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:20.249318Z","iopub.execute_input":"2023-09-13T06:47:20.249705Z","iopub.status.idle":"2023-09-13T06:47:20.256115Z","shell.execute_reply.started":"2023-09-13T06:47:20.249668Z","shell.execute_reply":"2023-09-13T06:47:20.255106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Autoencoders**\nAutoencoders are an unsupervised learning technique in which we leverage neural networks for the task of representation learning. Specifically, we'll design a neural network architecture such that we impose a bottleneck in the network which forces a compressed knowledge representation of the original input.\nAs visualized below, we can take an unlabeled dataset and frame it as a supervised learning problem tasked with outputting x_hat, a reconstruction of the original input x.\nNote: Here in our particular case we will try to reconstruct the low resolution images into their corresponding high resolution images.","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf","metadata":{}},{"cell_type":"code","source":"input_img = tf.keras.layers.Input(shape=(800, 1200, 3)) \n\nl1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(input_img)\nl2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l1)\nl3 = tf.keras.layers.MaxPool2D(padding='same')(l2)\nl3 = Dropout(0.3)(l3)\n\nl4 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l3)\nl5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l4)\nl6 = tf.keras.layers.MaxPool2D(padding='same')(l5)\n\nl7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l6)\n\nl8 = tf.keras.layers.UpSampling2D()(l7)\n\nl9 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l8)\nl10 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l9)\n\nl11 = tf.keras.layers.add([l10, l5])\n\nl12 = tf.keras.layers.UpSampling2D()(l11)\n\nl13 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l12)\nl14 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l13)\n\nl15 = tf.keras.layers.add([l14, l2])\n\ndecoded_image = tf.keras.layers.Conv2D(3, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu')(l15)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:20.260236Z","iopub.execute_input":"2023-09-13T06:47:20.261187Z","iopub.status.idle":"2023-09-13T06:47:25.267030Z","shell.execute_reply.started":"2023-09-13T06:47:20.261150Z","shell.execute_reply":"2023-09-13T06:47:25.266030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Model(inputs=(input_img), \n                                     outputs=decoded_image)\n\nmodel.compile( optimizer='Adam',  #'adadelta'\n                     loss='mean_absolute_error', #lr = 0.01, loss=\"perceptual_loss\",'mean_squared_error'/binary_crossentropy\n                     metrics=['accuracy'])  ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:25.268511Z","iopub.execute_input":"2023-09-13T06:47:25.268875Z","iopub.status.idle":"2023-09-13T06:47:25.296497Z","shell.execute_reply.started":"2023-09-13T06:47:25.268840Z","shell.execute_reply":"2023-09-13T06:47:25.295558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:25.297965Z","iopub.execute_input":"2023-09-13T06:47:25.298296Z","iopub.status.idle":"2023-09-13T06:47:25.345361Z","shell.execute_reply.started":"2023-09-13T06:47:25.298265Z","shell.execute_reply":"2023-09-13T06:47:25.344648Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count number of samples in train and val data\ntrain_samples = high_res_generator_train.samples\nval_samples = high_res_generator_val.samples","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:25.346325Z","iopub.execute_input":"2023-09-13T06:47:25.346650Z","iopub.status.idle":"2023-09-13T06:47:25.351006Z","shell.execute_reply.started":"2023-09-13T06:47:25.346619Z","shell.execute_reply":"2023-09-13T06:47:25.350150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preparing data for train and test to feed values in model.fit\ntrain_data = imageGenerator(train_image_zip)\nval_data = imageGenerator(val_image_zip)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:25.352260Z","iopub.execute_input":"2023-09-13T06:47:25.352582Z","iopub.status.idle":"2023-09-13T06:47:25.374286Z","shell.execute_reply.started":"2023-09-13T06:47:25.352550Z","shell.execute_reply":"2023-09-13T06:47:25.373489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply earlystopping, Modelcheckpoint and ReduceLRonplateau\nearly_stopper = EarlyStopping(monitor='val_loss', \n                              min_delta=0.01, \n                              patience=10, #50\n                              verbose=1, \n                              mode='min')\n\nmodel_checkpoint =  ModelCheckpoint(\n                                    'model.h5', \n                                    save_best_only = True)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.00000001)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T06:47:25.375274Z","iopub.execute_input":"2023-09-13T06:47:25.375587Z","iopub.status.idle":"2023-09-13T06:47:25.391770Z","shell.execute_reply.started":"2023-09-13T06:47:25.375557Z","shell.execute_reply":"2023-09-13T06:47:25.390773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://saturncloud.io/blog/understanding-keras-resolving-the-mismatch-between-stepsperepoch-and-imagedatagenerator-output/#:~:text=In%20Keras%2C%20steps_per_epoch%20is%20a,divided%20by%20the%20batch%20size.\n","metadata":{}},{"cell_type":"markdown","source":"Use of Steps_per_epoch\nhttps://datascience.stackexchange.com/questions/47405/what-to-set-in-steps-per-epoch-in-keras-fit-generator","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n                        train_data,\n                        steps_per_epoch=train_samples//batch_size,# math.ceil(train_samples//batch_size)\n                        validation_data=val_data,\n                        validation_steps=val_samples//batch_size,# math.ceil(val_samples//batch_size)\n                        epochs=10, #500\n                        callbacks=[early_stopper, model_checkpoint, learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:06:40.850159Z","iopub.execute_input":"2023-09-13T12:06:40.850447Z","iopub.status.idle":"2023-09-13T12:06:41.251516Z","shell.execute_reply.started":"2023-09-13T12:06:40.850420Z","shell.execute_reply":"2023-09-13T12:06:41.247691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Curve of loss and Accuracy\nplt.figure(figsize=(5,4))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\n\n\nplt.figure(figsize=(5,4))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Original Image, Ground Truth Image, Predicted Super Resolution Image\nn = 0\n\nfor img, out in val_image_zip:\n    pred = model.predict(img)\n    if n < 5:\n        fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n\n        axes[0].clear()\n        axes[0].set_title('Low Resolution Image', fontsize=10)\n        axes[0].imshow(img[0])\n        axes[0].axis('off')\n        \n        \n        axes[1].clear()\n        axes[1].set_title('High Resolution Image', fontsize=10)\n        axes[1].imshow(out[0])\n        axes[1].axis('off')\n        \n        axes[2].clear()\n        axes[2].set_title('Predicted High Resolution Image', fontsize=10)\n        axes[2].imshow(pred[0])\n        axes[2].axis('off')\n        \n\n        plt.pause(0.1)  # Pause briefly to allow the figure to update\n        plt.show()\n\n        n += 1\n    else:\n        break\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}